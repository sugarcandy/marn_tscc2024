import torch
import clip
from PIL import Image
import torch.nn.functional as F
import torch.nn as nn
from graph_part.align_loss import *

# device = "cuda" if torch.cuda.is_available() else "cpu"
# model, preprocess = clip.load("RN50", device=device)
# # model.visual.attnpool.c_proj=torch.nn.Linear(2048,300,dtype=torch.float16).cuda()
# # torch.nn.init.eye_(model.visual.attnpool.c_proj.weight)
# # model.visual.attnpool.c_proj.weight.to(torch.float32)
# # print(model.visual)
# print(clip.available_models())
#
# image = preprocess(Image.open("CLIP.png")).unsqueeze(0).to(device)
# # text = clip.tokenize(["a diagram", "a dog", "a cat"]).to(device)
# with torch.no_grad():
#     image_features = model.encode_image(image)
#     print(image_features)
#     print(image)
#     text_features = model.encode_text(text)
#
#     logits_per_image, logits_per_text = model(image, text)
#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()
#
# print("Label probs:", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]
x = torch.tensor([[[-4.1469e-04, -0.0000e+00, -8.1630e-05, -2.3779e-01, -0.0000e+00,
                    -0.0000e+00, -1.4001e-01, -2.4274e-01, -0.0000e+00, -1.3543e-03,
                    -1.1788e-01, -0.0000e+00, -1.7492e-01, -9.6562e-04, -0.0000e+00,
                    -0.0000e+00, -2.2799e-01, -0.0000e+00, -4.0609e-04, -2.1658e-01,
                    -2.2745e-01, 7.8127e+00, -2.6748e-02, -0.0000e+00, -0.0000e+00,
                    9.4366e+00, -6.9294e-02, -0.0000e+00, -7.2981e-05, -2.2836e-01,
                    -4.7658e-05, -0.0000e+00, -0.0000e+00, -1.1425e-03, -1.4074e-01,
                    -2.3900e-01, -4.3810e-05, -0.0000e+00, -2.0044e-03, -0.0000e+00,
                    -1.0192e-04, -7.0452e-04, -3.8529e-05, -2.8150e-02, -0.0000e+00,
                    -2.3305e-01, -2.3804e-01, -3.9390e-06, -3.7751e-04, -0.0000e+00,
                    -1.3635e-01, -5.7065e-04, -0.0000e+00, -1.7821e-01, -1.3405e-03,
                    -6.9557e-03, -1.0551e-02, -6.3239e-05, -7.1265e-02, -0.0000e+00,
                    -4.6390e-02, -1.5622e-02, -2.4274e-01, -3.5382e-06, -9.6157e-02,
                    -4.7161e-04, -0.0000e+00, -2.2852e-07, -4.5216e-05, -1.5376e-01,
                    -1.6721e-03, -5.1884e-02, -0.0000e+00, -0.0000e+00, -1.5771e-05,
                    -7.1999e-03, -2.3285e-01, -2.1630e-01, -0.0000e+00, -2.4252e-01,
                    -1.5000e-01, -1.8576e-01, -2.0631e-01, -3.7409e-06, -6.2497e-02,
                    -3.9169e-03, -0.0000e+00, -0.0000e+00, -8.6764e-02, -0.0000e+00,
                    -2.0765e-01, -1.7384e-01, 5.9667e-03, -0.0000e+00, -2.1220e-03,
                    0.0000e+00, 2.3810e-01, 2.1106e-01, 4.8753e-01, 1.5207e-01,
                    1.7547e-01, 6.6950e-01, 3.7857e-01, 3.4109e-01, 2.7733e-01],
                   [-4.1469e-04, -0.0000e+00, -8.1630e-05, -2.3779e-01, -0.0000e+00,
                    -0.0000e+00, -1.4001e-01, -2.4274e-01, -0.0000e+00, -1.3543e-03,
                    -1.1788e-01, -0.0000e+00, -1.7492e-01, -9.6562e-04, -0.0000e+00,
                    -0.0000e+00, -2.2799e-01, -0.0000e+00, -4.0609e-04, -2.1658e-01,
                    -2.2745e-01, 7.8127e+00, -2.6748e-02, -0.0000e+00, -0.0000e+00,
                    9.4366e+00, -6.9294e-02, -0.0000e+00, -7.2981e-05, -2.2836e-01,
                    -4.7658e-05, -0.0000e+00, -0.0000e+00, -1.1425e-03, -1.4074e-01,
                    -2.3900e-01, -4.3810e-05, -0.0000e+00, -2.0044e-03, -0.0000e+00,
                    -1.0192e-04, -7.0452e-04, -3.8529e-05, -2.8150e-02, -0.0000e+00,
                    -2.3305e-01, -2.3804e-01, -3.9390e-06, -3.7751e-04, -0.0000e+00,
                    -1.3635e-01, -5.7065e-04, -0.0000e+00, -1.7821e-01, -1.3405e-03,
                    -6.9557e-03, -1.0551e-02, -6.3239e-05, -7.1265e-02, -0.0000e+00,
                    -4.6390e-02, -1.5622e-02, -2.4274e-01, -3.5382e-06, -9.6157e-02,
                    -4.7161e-04, -0.0000e+00, -2.2852e-07, -4.5216e-05, -1.5376e-01,
                    -1.6721e-03, -5.1884e-02, -0.0000e+00, -0.0000e+00, -1.5771e-05,
                    -7.1999e-03, -2.3285e-01, -2.1630e-01, -0.0000e+00, -2.4252e-01,
                    -1.5000e-01, -1.8576e-01, -2.0631e-01, -3.7409e-06, -6.2497e-02,
                    -3.9169e-03, -0.0000e+00, -0.0000e+00, -8.6764e-02, -0.0000e+00,
                    -2.0765e-01, -1.7384e-01, 5.9667e-03, -0.0000e+00, -2.1220e-03,
                    0.0000e+00, 2.3810e-01, 2.1106e-01, 4.8753e-01, 1.5207e-01,
                    1.7547e-01, 6.6950e-01, 3.7857e-01, 3.4109e-01, 2.7733e-01]],

                  [[-1.0456e-03, -1.8240e-01, -2.5096e-03, -0.0000e+00, -0.0000e+00,
                    -9.7620e-02, -6.4174e-03, -2.2311e-03, -0.0000e+00, -2.2571e-01,
                    -2.5144e-03, -0.0000e+00, -0.0000e+00, -8.3953e-02, 0.0000e+00,
                    -3.8589e-03, -1.9545e-02, -2.2896e-01, -1.2987e-01, -4.2371e-04,
                    -3.5406e-02, 7.5679e-01, -0.0000e+00, -1.0969e-03, -0.0000e+00,
                    -2.3163e-01, -4.5301e-06, -1.9980e-01, -2.2254e-01, -6.6142e-03,
                    -8.0696e-02, -0.0000e+00, -9.3673e-02, -2.2305e-02, -0.0000e+00,
                    6.3800e+00, -0.0000e+00, -2.1316e-01, -2.3945e-01, -7.1113e-05,
                    -1.8820e-01, -0.0000e+00, -0.0000e+00, -1.8664e-02, -0.0000e+00,
                    -0.0000e+00, -3.6109e-04, -9.9761e-03, -0.0000e+00, 0.0000e+00,
                    -0.0000e+00, -6.3522e-02, -1.3311e-01, -1.4091e-05, -4.5289e-07,
                    -1.3715e-01, -1.3340e-02, -1.7979e-01, -0.0000e+00, -1.7597e-01,
                    -4.7850e-03, -1.9775e-03, -1.3158e-05, -0.0000e+00, -0.0000e+00,
                    -4.8832e-03, -0.0000e+00, -3.0730e-02, -3.1412e-04, -1.0724e-04,
                    -1.3771e-01, 3.1006e+00, -1.7937e-03, -1.9286e-02, -1.3492e-02,
                    -1.6572e-01, 5.0602e+00, -3.2678e-03, -1.4603e-01, -8.4940e-02,
                    -2.1059e-03, -2.2787e-01, -3.7509e-02, -0.0000e+00, -0.0000e+00,
                    -5.7095e-06, -1.6233e-02, -1.1180e-01, -3.6111e-02, -6.0606e-03,
                    0.0000e+00, -0.0000e+00, 0.0000e+00, -1.4254e-02, -6.8886e-05,
                    5.0880e-02, 4.2052e-01, 5.2086e-01, 1.6973e-01, 3.2645e-01,
                    4.7859e-01, 7.2998e-02, 5.2343e-01, 2.7686e-01, 2.6292e-01],
                   [-1.0456e-03, -1.8240e-01, -2.5096e-03, -0.0000e+00, -0.0000e+00,
                    -9.7620e-02, -6.4174e-03, -2.2311e-03, -0.0000e+00, -2.2571e-01,
                    -2.5144e-03, -0.0000e+00, -0.0000e+00, -8.3953e-02, 0.0000e+00,
                    -3.8589e-03, -1.9545e-02, -2.2896e-01, -1.2987e-01, -4.2371e-04,
                    -3.5406e-02, 7.5679e-01, -0.0000e+00, -1.0969e-03, -0.0000e+00,
                    -2.3163e-01, -4.5301e-06, -1.9980e-01, -2.2254e-01, -6.6142e-03,
                    -8.0696e-02, -0.0000e+00, -9.3673e-02, -2.2305e-02, -0.0000e+00,
                    6.3800e+00, -0.0000e+00, -2.1316e-01, -2.3945e-01, -7.1113e-05,
                    -1.8820e-01, -0.0000e+00, -0.0000e+00, -1.8664e-02, -0.0000e+00,
                    -0.0000e+00, -3.6109e-04, -9.9761e-03, -0.0000e+00, 0.0000e+00,
                    -0.0000e+00, -6.3522e-02, -1.3311e-01, -1.4091e-05, -4.5289e-07,
                    -1.3715e-01, -1.3340e-02, -1.7979e-01, -0.0000e+00, -1.7597e-01,
                    -4.7850e-03, -1.9775e-03, -1.3158e-05, -0.0000e+00, -0.0000e+00,
                    -4.8832e-03, -0.0000e+00, -3.0730e-02, -3.1412e-04, -1.0724e-04,
                    -1.3771e-01, 3.1006e+00, -1.7937e-03, -1.9286e-02, -1.3492e-02,
                    -1.6572e-01, 5.0602e+00, -3.2678e-03, -1.4603e-01, -8.4940e-02,
                    -2.1059e-03, -2.2787e-01, -3.7509e-02, -0.0000e+00, -0.0000e+00,
                    -5.7095e-06, -1.6233e-02, -1.1180e-01, -3.6111e-02, -6.0606e-03,
                    0.0000e+00, -0.0000e+00, 0.0000e+00, -1.4254e-02, -6.8886e-05,
                    5.0880e-02, 4.2052e-01, 5.2086e-01, 1.6973e-01, 3.2645e-01,
                    4.7859e-01, 7.2998e-02, 5.2343e-01, 2.7686e-01, 2.6292e-01]]])

y = torch.tensor([1, 0], dtype=torch.long)

loss_cons = SupConLoss(temperature=1.0)
out=loss_cons(x,y)
print(out)